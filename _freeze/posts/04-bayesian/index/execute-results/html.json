{
  "hash": "9b60394348db94869d90ec306e2bb58c",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Mini Project 4\"\nauthor: \"Abby Sikora\"\ndate: \"2025-04-28\"\n---\n\n\n\n“I have followed all rules for collaboration for this project, and I have not used generative AI on this project.”\n\nThe question of interest for this project is can we predict the probability that Nadal wins a point on his own serve against his primary rival, Novak Djokovic, at the French Open (the most prestigious clay court tournament in the world). We will answer this by using 3 different priors, a noninformative, and two informative priors based on preexisting knowledge or not. We will then update our posterior distributions to take into account the 2020 French open to get a more accurate estimate at the end, finishing off the project with 3 different credible posterior intervals with 90% confidence. \n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n```\n\n\n:::\n\n```{.r .cell-code}\nps <- seq(0, 1, length.out = 1000)\n\nnoninformative_alpha <- 1\nnoninformative_beta <- 1\n\nnoninformative_prior <- dbeta(ps,\n                              noninformative_alpha, noninformative_beta)\n```\n:::\n\n\n\nThe fist prior is completely non-informative. This means we have no preexisting knowledge of Nadal's performance or any idea of how it will go, so we will use a Beta(1, 1), which is equal to Uniform(0, 1) where 0 represents a loss and 1 represents a win on a serve. Uniform(0,1) assigns the same probability for every value in the interval, not making any assumptions about the points won on Nadal's serves. \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntarget_mean <- 0.697\n\nalphas <- seq(0.1, 100, length.out = 500)\nbetas <- 0.303 * alphas / 0.697\n\nparam_df <- tibble(alphas, betas)\nparam_df <- param_df |> mutate(vars = \n                    (alphas * betas) / ((alphas + betas)^2 * (alphas + betas + 1)))\n\n\ntarget_var <- 0.05657 ^2\n\nparam_df <- param_df |> mutate(dist_to_target = abs(vars - target_var))\nparam_df\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 500 × 4\n   alphas  betas   vars dist_to_target\n    <dbl>  <dbl>  <dbl>          <dbl>\n 1  0.1   0.0435 0.185          0.181 \n 2  0.300 0.131  0.148          0.144 \n 3  0.500 0.218  0.123          0.120 \n 4  0.701 0.305  0.105          0.102 \n 5  0.901 0.392  0.0921         0.0889\n 6  1.10  0.479  0.0819         0.0787\n 7  1.30  0.566  0.0737         0.0705\n 8  1.50  0.653  0.0670         0.0638\n 9  1.70  0.740  0.0614         0.0582\n10  1.90  0.827  0.0566         0.0534\n# ℹ 490 more rows\n```\n\n\n:::\n\n```{.r .cell-code}\nparam_df |> filter(dist_to_target == min(dist_to_target))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 4\n  alphas betas    vars dist_to_target\n   <dbl> <dbl>   <dbl>          <dbl>\n1   45.3  19.7 0.00320     0.00000310\n```\n\n\n:::\n\n```{.r .cell-code}\ninformative_alpha <- 45.345\ninformative_beta <- 19.713\n```\n:::\n\n\n\nFor the second prior, it is informative based on a clay-court match the two played in the previous year. In that match, Nadal won 46 out of 66 points on his own serve. The standard error of this estimate is 0.05657. Again, I chose a beta distribution but to find the alphas and betas I worked backwards from the mean equation for a beta distribution (E(x) = alpha/alpha+beta). Once I found Beta = (0.303 * alpha)/0.697, I assigned a sequence of values for alpha and used this formula to solve for beta. In addition to this, I also incorporated the standard error into a target variance, by squaring it and made sure the parameters were adjusted accordingly.  \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntarget_mean <- 0.75\n\nalphas <- seq(0.1, 100, length.out = 500)\nbetas <- 0.25 * alphas / 0.75\n\nparam_df <- tibble(alphas, betas)\nparam_df <- param_df |> mutate(vars = \n                    (alphas * betas) / ((alphas + betas)^2 * (alphas + betas + 1)))\n\n\ntarget_var <- 0.05 ^2\n\nparam_df <- param_df |> mutate(dist_to_target = abs(vars - target_var))\nparam_df\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 500 × 4\n   alphas  betas   vars dist_to_target\n    <dbl>  <dbl>  <dbl>          <dbl>\n 1  0.1   0.0333 0.165          0.163 \n 2  0.300 0.100  0.134          0.131 \n 3  0.500 0.167  0.112          0.110 \n 4  0.701 0.234  0.0969         0.0944\n 5  0.901 0.300  0.0852         0.0827\n 6  1.10  0.367  0.0760         0.0735\n 7  1.30  0.434  0.0686         0.0661\n 8  1.50  0.500  0.0625         0.0600\n 9  1.70  0.567  0.0574         0.0549\n10  1.90  0.634  0.0530         0.0505\n# ℹ 490 more rows\n```\n\n\n:::\n\n```{.r .cell-code}\nparam_df |> filter(dist_to_target == min(dist_to_target))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 4\n  alphas betas    vars dist_to_target\n   <dbl> <dbl>   <dbl>          <dbl>\n1   55.6  18.5 0.00250     0.00000246\n```\n\n\n:::\n\n```{.r .cell-code}\ninformative_alpha2 <- 55.555\ninformative_beta2 <- 18.519\n```\n:::\n\n\n\nFor the third prior, it is another informative one based on a sports announcer, who claims that they think Nadal wins about 75% of the points on his serve against Djokovic. They are also “almost sure” that Nadal wins no less than 70% of his points on serve against Djokovic. To figure out this final prior, I used the same strategy for the second prior with a little bit different logic. For this prior it will be another Beta, and we have once again been given the proportion of wins and can work backwards with the mean formula to get the betas = (0.25 * alphas) / 0.75. To make sure that this mean does not go below 70%, I chose a standard error of 0.05 and proceeded with the same strategy as the first prior.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ninformative_prior <- dbeta(ps, informative_alpha,\n                           informative_beta)\ninformative_prior2 <- dbeta(ps, informative_alpha2,\n                           informative_beta2)\n\nnoninformative_prior <- dbeta(ps,\n                              noninformative_alpha, noninformative_beta)\n\n\nprior_plot <- tibble(ps, informative_prior, noninformative_prior, informative_prior2) |>\n  pivot_longer(2:4, names_to = \"prior_type\", values_to = \"density\")\n\nggplot(data = prior_plot, aes(x = ps, y = density, colour = prior_type)) +\n  geom_line() +\n  scale_colour_viridis_d(end = 0.9) +\n  theme_minimal() +\n  labs(x = \"p\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ninformative_alpha_post <- informative_alpha + 56\ninformative_beta_post <- informative_beta + (84 - 56)\ninformative_post <- dbeta(ps, informative_alpha_post,\n                          informative_beta_post)\n\n#posterior mean for 2nd prior\ninformative_alpha_post / (informative_alpha_post + informative_beta_post)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.6799031\n```\n\n\n:::\n\n```{.r .cell-code}\ninformative_alpha_post2 <- informative_alpha2 + 56\ninformative_beta_post2 <- informative_beta2 + (84 - 56)\ninformative_post2 <- dbeta(ps, informative_alpha_post2,\n                          informative_beta_post2)\n#posterior mean for 3rd prior\ninformative_alpha_post2 / (informative_alpha_post2 + informative_beta_post2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.7057138\n```\n\n\n:::\n\n```{.r .cell-code}\n## CHANGE THESE\nnoninformative_alpha_post <- noninformative_alpha + 56\nnoninformative_beta_post <- noninformative_beta + (84 - 56)\nnoninformative_post <- dbeta(ps, noninformative_alpha_post,\n                             noninformative_beta_post)\n#posterior mean for noninformative prior\nnoninformative_alpha_post / (noninformative_alpha_post + noninformative_beta_post)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.6627907\n```\n\n\n:::\n\n```{.r .cell-code}\nplot_df <- tibble(ps,\n                     informative_post, noninformative_post, informative_post2) |>\n  pivot_longer(2:4, names_to = \"distribution\", values_to = \"density\")\n\nggplot(data = plot_df, aes(x = ps, y = density, colour = distribution)) +\n  geom_line() +\n  scale_colour_viridis_d(end = 0.9) +\n  theme_minimal() +\n  labs(x = \"p\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\nCredible Intervals\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnoninformative_post_int <- qbeta(c(0.05, 0.95), 57, 29)\n\ninformative_post_int <- qbeta(c(0.05, 0.95), informative_alpha + 56, informative_beta + (84 - 56))\n\ninformative_post_int2 <- qbeta(c(0.05, 0.95), informative_alpha2 + 56, informative_beta2 + (84 - 56))\n```\n:::\n\n\n\nThe three posteriors that I ended up with were all a little different from eachother. The noninformative prior gave the lowest posterior mean of 0.66279, followed by the posterior mean for the prior based on the previous year at 0.6768, then finally the highest mean of 0.6799 belongs to the prior based on the announcers estimate. They are all different from eachother slightly because of the information we used to obtain the priors. The non-informative prior is the closest to the actual mean from 2020, 0.6667. The variance of the non informative prior is the highest based on the plot of the posterior distributions. Between the two informative posteriors, the one based on the announcer's prediction has a very slightly lower variance than the one based on data from the previous year, yet this is barely noticeable and at the end of the day they look like they have almost exactly the same variance. If I had to choose one posterior to rely on, I would choose the one based on the previous year's data. There is too much variance within the non-informative posterior. Because the variances for the two informative posteriors are so similar, I won't take that very strongly into consideration. Between the two, the one based on the previous years data has a posterior mean closer to the 2020 match, and I would pick that one. \n\nIn conclusion, I found that using an informative prior in this case helps make the estimates closer to the actual win percentage with a lower variance. There is a lot more variabliity in a non informative prior because you are assuming there is no difference between players skill level, which is never the case in these scenarios. \n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}